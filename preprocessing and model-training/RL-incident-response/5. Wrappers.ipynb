{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from pprint import pprint\n",
    "from os.path import dirname\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents.Wrappers import *\n",
    "from CybORG.Simulator.Scenarios import FileReaderScenarioGenerator\n",
    "\n",
    "path = inspect.getfile(CybORG)\n",
    "path = dirname(path) + f'/Simulator/Scenarios/scenario_files/Scenario1b.yaml'\n",
    "sg = FileReaderScenarioGenerator(path)\n",
    "\n",
    "cyborg = CybORG(scenario_generator=sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our demonstrations have mostly been with native CybORG, but we will now demonstrate how AI can be assisted by the use of Wrappers.\n",
    "\n",
    "Our first Wrapper is EnumActionWrapper, which calculates all the possible actions and returns the action space as the number of such actions. This gives a relatively large number of actions for both Blue and Red team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnumActionWrapper(cyborg)\n",
    "\n",
    "results = env.reset(agent='Blue')\n",
    "action_space = results.action_space\n",
    "print('Blue action space:',action_space)\n",
    "\n",
    "results = env.reset(agent='Red')\n",
    "action_space = results.action_space\n",
    "print('Red action space:', action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar thing for the observation space. The FlatFixedWrapper parses the internal state of CybORG and turns it into a list of floats, which can easily be converted into a vector. Unfortunately, this vector is extremely long, with length over 11,000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FixedFlatWrapper(cyborg)\n",
    "\n",
    "results = env.reset()\n",
    "obs = results.observation\n",
    "print(type(obs))\n",
    "print(len(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAIGymWrapper converts the output of FlatFixedWrapper to a numpy array as well as conforming to other parts of the OpenAI Gym API. It requires FlatFixedWrapper in order to function and should always be the outermost of the provided wrappers. Note it also performs the functions of the EnumActionWrapper You must also specify an agent parameter and explitly specify the environment parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrappers = FixedFlatWrapper(cyborg)\n",
    "env = OpenAIGymWrapper(env=wrappers,agent_name='Blue')\n",
    "\n",
    "obs = env.reset()\n",
    "print('Observation:',obs)\n",
    "print(73*'-')\n",
    "print('Action_Space:',env.action_space)\n",
    "print(73*'-')\n",
    "print('Observation Space:',env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Table Wrappers' attempt to use basic logic to infer a human-friendly picture of the state by keeping track of past observations. This allows for a greatly simplified state space and much greater human readibility. However, it mercilessly exploits the current limitations of Scenario 1b and thus would have limited use on real-world cyber problems.\n",
    "\n",
    "The first wrapper is the TrueTableWrapper, which modifies the get_agent_state method to return the true state in the form of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrueTableWrapper(cyborg)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "true_table = env.get_agent_state('True')\n",
    "print(true_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BlueTableWrapper provides similar functionality for the blue observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG.Agents import B_lineAgent\n",
    "from CybORG.Simulator.Actions import Sleep\n",
    "cyborg = CybORG(sg, agents={'Red':B_lineAgent()})\n",
    "\n",
    "env = BlueTableWrapper(cyborg)\n",
    "\n",
    "results = env.reset(agent='Blue')\n",
    "\n",
    "for i in range(3):\n",
    "    results = env.step(agent='Blue')\n",
    "    blue_obs = results.observation\n",
    "    print(blue_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table can also be converted into a vector. This is done by setting the output_mode parameter to 'vector'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlueTableWrapper(cyborg,output_mode='vector')\n",
    "\n",
    "env.reset(agent='Blue')\n",
    "for i in range(3):\n",
    "    results = env.step(agent='Blue')\n",
    "    blue_obs = results.observation\n",
    "    print(blue_obs)\n",
    "    print(76*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RedTableWrapper is the Red Team version of the BlueTableWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RedTableWrapper(cyborg)\n",
    "\n",
    "results = env.reset(agent='Red')\n",
    "print(results.observation)\n",
    "\n",
    "for i in range(3):\n",
    "    results = env.step(agent='Red')\n",
    "    red_obs = results.observation\n",
    "    print(red_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PettingZoo Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 3 utilises the PettingZoo Wrapper for multiagent reinforcement learning. See Notebook 1. Introduction for how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
